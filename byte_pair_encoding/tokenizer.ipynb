{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strings in python are sequences of unicode code points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('h') # ord function to get unidode code points in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the problem with this type of encoding is\n",
    "1. vocab size is too short\n",
    "2. unicode standard is dynamic and new characters keeps on getting added based on new scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the need is something more stable\n",
    "therefore, encodings.\n",
    "\n",
    "3 types offered by the uincode consortium - utf-8, utf-16, utf-32\n",
    "\n",
    "out of these, utf-8 is the most preferred one. But it generates variable byte stream which could be a problem when considering the language models context window which is limited for computational reasons. This will effect the next word prediction task\n",
    "\n",
    "So the aim is to stick to the utf-8 encoding but also want to attend larger vocab size during attention and tune it as a hyperparameter.\n",
    "\n",
    "Compress the bytes generated by utf-8 encoding\n",
    "\n",
    "Therefore, the solution is byte pair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea is, we'll take the sequence in terms of bytes and pair them iteratively based on the frequency count thereby minting new tokens and thus increasing the vocabulaory size in the process. This compresses the input sequence and thus the token thereby having a knowledge rich vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw text\n",
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "raw_bytes = text.encode(\"utf-8\")\n",
    "print(raw_bytes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
